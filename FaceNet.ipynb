{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5696d868a0f64e1caa6599a1fb4d9b6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f91eb5a21b340b198f2f974decc4e14",
              "IPY_MODEL_e01a499d88644006aeac6cf5dadef440",
              "IPY_MODEL_cc5ea44a255d4028a617ae2539a71243"
            ],
            "layout": "IPY_MODEL_35df9efec8ee4ad9ad5520a6f70211d5"
          }
        },
        "3f91eb5a21b340b198f2f974decc4e14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c4fde29ce054c61a6a63a35c0455247",
            "placeholder": "​",
            "style": "IPY_MODEL_f00cabce1ccd4ea0b58771f06eabb2d9",
            "value": "100%"
          }
        },
        "e01a499d88644006aeac6cf5dadef440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68e069ba96e443aca82f67455562b04a",
            "max": 111898327,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d694e06996b04c1da77d6bd8a03c67d9",
            "value": 111898327
          }
        },
        "cc5ea44a255d4028a617ae2539a71243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a65afc559c9d4d35830058f85440aaf1",
            "placeholder": "​",
            "style": "IPY_MODEL_8058654bc1fb4c8094500404a1876be1",
            "value": " 107M/107M [00:00&lt;00:00, 252MB/s]"
          }
        },
        "35df9efec8ee4ad9ad5520a6f70211d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c4fde29ce054c61a6a63a35c0455247": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f00cabce1ccd4ea0b58771f06eabb2d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68e069ba96e443aca82f67455562b04a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d694e06996b04c1da77d6bd8a03c67d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a65afc559c9d4d35830058f85440aaf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8058654bc1fb4c8094500404a1876be1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Face Recognition dengan Akurasi\n",
        "## Menggunakan FaceNet (VGGFace2) + Augmentation + MTCNN Alignment  \n",
        "### Dataset dari Roboflow\n",
        "\n",
        "**Tujuan notebook ini:**\n",
        "- Download dataset face recognition dari Roboflow (format folder)\n",
        "- Training FaceNet dengan augmentasi\n",
        "- Tuning threshold otomatis\n",
        "- Testing + visualisasi\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "CdhGwa-yQ0a3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install & Import Library"
      ],
      "metadata": {
        "id": "XwhHNHdRQ6h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install library yang dibutuhkan\n",
        "!pip install -q roboflow facenet_pytorch opencv-python-headless albumentations insightface mtcnn tqdm\n",
        "\n",
        "# Import semua library\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
        "from torchvision import transforms\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Semua library berhasil di-import!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgvQQmssQ9BG",
        "outputId": "66c1d5ec-3ddb-4390-cbfa-b119f282187f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semua library berhasil di-import!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Download Dataset dari Roboflow"
      ],
      "metadata": {
        "id": "ZEVKRCVyRDgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "\n",
        "rf = Roboflow(api_key=\"QOd5ldAdjiaehHn5m6WC\")\n",
        "project = rf.workspace(\"dentalogic8\").project(\"face-sz6g1\")\n",
        "version = project.version(1)\n",
        "# =================================================================\n",
        "\n",
        "# Download dalam format \"folder\" (paling cocok untuk face recognition)\n",
        "dataset = version.download(\"folder\")\n",
        "\n",
        "# Cek lokasi dataset\n",
        "dataset_location = dataset.location\n",
        "print(f\"\\nDataset berhasil di-download ke:\\n {dataset_location}\\n\")\n",
        "print(\"Isi folder train:\")\n",
        "!ls -l \"{dataset_location}/train\" | head -20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DitGP8QgRJUU",
        "outputId": "d155fd5c-d0f3-469a-c91e-51f372140ada"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Face-1 to folder:: 100%|██████████| 4895/4895 [00:00<00:00, 18615.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Face-1 in folder:: 100%|██████████| 824/824 [00:00<00:00, 13177.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset berhasil di-download ke:\n",
            " /content/Face-1\n",
            "\n",
            "Isi folder train:\n",
            "total 220\n",
            "drwxr-xr-x 2 root root 4096 Nov 30 15:01 Aan Kristian Sitinjak\n",
            "drwxr-xr-x 2 root root 4096 Nov 30 15:01 Abeloisa Pardosi\n",
            "drwxr-xr-x 2 root root 4096 Nov 30 15:01 Adriano Lumbantoruan\n",
            "drwxr-xr-x 2 root root 4096 Nov 30 15:01 Agnes Sidabutar\n",
            "drwxr-xr-x 2 root root 4096 Nov 30 15:01 Andri Sigiro\n",
            "drwxr-xr-x 2 root root 4096 Nov 30 15:01 Angelika Simamora\n",
            "drwxr-xr-x 2 root root 4096 Nov 30 15:01 Anno Siregar\n",
            "drwxr-xr-x 2 root root 4096 Nov 30 15:01 Antonia Manalu\n",
            "drwxr-xr-x 2 root root 4096 Nov 30 15:01 Arnold Daniel Manalu\n",
            "drwxr-xr-x 2 root root 4096 Nov 30 15:01 Ayu Sinaga\n",
            "drwxr-xr-x 2 root root 4096 Nov 30 15:01 Bowo Manalu\n",
            "drwxr-xr-x 2 root root 4096 Nov 30 15:01 Chelsea Sianipar\n",
            "drwxr-xr-x 2 root root 4096 Nov 30 15:01 Chenit Sirait\n",
            "drwxr-xr-x 2 root root 4096 Nov 30 15:01 Cheryl\n",
            "drwxr-xr-x 2 root root 4096 Nov 30 15:01 Daniel Ginting\n",
            "drwxr-xr-x 2 root root 4096 Nov 30 15:01 Diva Marbun\n",
            "drwxr-xr-x 2 root root 4096 Nov 30 15:01 Estina Pangaribuan\n",
            "drwxr-xr-x 2 root root 4096 Nov 30 15:01 Febrina Sihombing\n",
            "drwxr-xr-x 2 root root 4096 Nov 30 15:01 Febyanti Hutahaean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Konfigurasi Utama"
      ],
      "metadata": {
        "id": "mSYQYpwvRQHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- CONFIG (SESUAIKAN) -------------------\n",
        "DATASET_ROOT = f\"{dataset.location}/train\"   # Folder train dari dataset Roboflow\n",
        "RESULT_FOLDER = \"hasil_validasi\"\n",
        "os.makedirs(RESULT_FOLDER, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device yang digunakan: {DEVICE}\")\n",
        "\n",
        "# Augmentasi berat per foto (semakin tinggi = semakin akurat, makin lama juga)\n",
        "AUG_PER_IMAGE = 50\n",
        "USE_MTCNN_ALIGN = True    # True = paling akurat (alignment berdasarkan landmark)\n",
        "USE_HAAR_CROP = True      # Kalau MTCNN gagal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFajonnARTyE",
        "outputId": "8c68d147-e5ea-436e-9063-42fa177d88cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device yang digunakan: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Haar Cascade (Fallback Face Detection)"
      ],
      "metadata": {
        "id": "qWQbkI14RjD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Haar Cascade untuk crop wajah manual (cadangan)\n",
        "haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "def crop_face_haar(img_cv2):\n",
        "    gray = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2GRAY)\n",
        "    faces = haar_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(100,100))\n",
        "    if len(faces) > 0:\n",
        "        x, y, w, h = sorted(faces, key=lambda x: x[2]*x[3], reverse=True)[0]\n",
        "        margin = int(w * 0.3)\n",
        "        x = max(x - margin, 0)\n",
        "        y = max(y - margin, 0)\n",
        "        w += 2 * margin\n",
        "        h += 2 * margin\n",
        "        return img_cv2[y:y+h, x:x+w]\n",
        "    return None"
      ],
      "metadata": {
        "id": "vYGQEoHWRlOf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. MTCNN untuk Face Alignment"
      ],
      "metadata": {
        "id": "KYhN931iRo8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MTCNN untuk alignment wajah (160x160) - sangat meningkatkan akurasi\n",
        "mtcnn = MTCNN(image_size=160, margin=0, keep_all=False, device=DEVICE) if USE_MTCNN_ALIGN else None"
      ],
      "metadata": {
        "id": "ZMxgGp3sRtsZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Augmentation dengan Albumentations"
      ],
      "metadata": {
        "id": "dV5FM-IaRwEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aug_heavy = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    A.Rotate(limit=25, p=0.7),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=0.8),\n",
        "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=30, p=0.7),\n",
        "    A.GaussNoise(var_limit=(10, 50), p=0.4),\n",
        "    A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.3),\n",
        "    A.RandomGamma(gamma_limit=(70, 130), p=0.5),\n",
        "    A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.4),\n",
        "    A.GaussianBlur(blur_limit=(3,7), p=0.3),\n",
        "    A.MotionBlur(blur_limit=7, p=0.2),\n",
        "    A.RandomShadow(p=0.3),\n",
        "    A.RandomFog(p=0.2),\n",
        "    A.RandomRain(p=0.1),\n",
        "    A.CLAHE(clip_limit=3.0, p=0.5),\n",
        "    A.Solarize(threshold=128, p=0.2),\n",
        "    A.Posterize(p=0.2),\n",
        "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=20, border_mode=0, p=0.8),\n",
        "], p=1.0)\n",
        "\n",
        "transform_to_tensor = ToTensorV2()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QnAj88rR2Uw",
        "outputId": "bb0cd101-41dc-4f2c-84c8-f018661d8738"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "Argument(s) 'threshold' are not valid for transform Solarize\n",
            "ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Load Model FaceNet (Pretrained VGGFace2)"
      ],
      "metadata": {
        "id": "lDSwFUcNR9-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading FaceNet (InceptionResnetV1) pretrained on VGGFace2...\")\n",
        "model = InceptionResnetV1(pretrained='vggface2').eval().to(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "5696d868a0f64e1caa6599a1fb4d9b6a",
            "3f91eb5a21b340b198f2f974decc4e14",
            "e01a499d88644006aeac6cf5dadef440",
            "cc5ea44a255d4028a617ae2539a71243",
            "35df9efec8ee4ad9ad5520a6f70211d5",
            "8c4fde29ce054c61a6a63a35c0455247",
            "f00cabce1ccd4ea0b58771f06eabb2d9",
            "68e069ba96e443aca82f67455562b04a",
            "d694e06996b04c1da77d6bd8a03c67d9",
            "a65afc559c9d4d35830058f85440aaf1",
            "8058654bc1fb4c8094500404a1876be1"
          ]
        },
        "id": "WMc_eTBwSCBF",
        "outputId": "4aa50a72-195f-4620-b6bc-1e72d40c199e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading FaceNet (InceptionResnetV1) pretrained on VGGFace2...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/107M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5696d868a0f64e1caa6599a1fb4d9b6a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Fungsi Ekstraksi Embedding (dengan Fallback)"
      ],
      "metadata": {
        "id": "A4AlN7fjSIny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(img_pil_or_cv2):\n",
        "    # Konversi ke PIL jika input dari OpenCV\n",
        "    if isinstance(img_pil_or_cv2, np.ndarray):\n",
        "        img = Image.fromarray(cv2.cvtColor(img_pil_or_cv2, cv2.COLOR_BGR2RGB))\n",
        "    else:\n",
        "        img = img_pil_or_cv2.convert('RGB')\n",
        "\n",
        "    # 1. Prioritas tertinggi: MTCNN alignment\n",
        "    if USE_MTCNN_ALIGN and mtcnn is not None:\n",
        "        try:\n",
        "            img_aligned = mtcnn(img)\n",
        "            if img_aligned is not None:\n",
        "                img_tensor = img_aligned.unsqueeze(0).to(DEVICE)\n",
        "                with torch.no_grad():\n",
        "                    emb = model(img_tensor).cpu().numpy()[0]\n",
        "                return emb\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # 2. Fallback: Crop manual dengan Haar + augmentasi berat\n",
        "    img_cv2 = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
        "    if USE_HAAR_CROP:\n",
        "        cropped = crop_face_haar(img_cv2)\n",
        "        if cropped is not None:\n",
        "            img_cv2 = cropped\n",
        "\n",
        "    img_pil = Image.fromarray(cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB))\n",
        "    aug_img = aug_heavy(image=np.array(img_pil))['image']\n",
        "    tensor = transform_to_tensor(image=aug_img)['image'].unsqueeze(0).to(DEVICE)\n",
        "    tensor = tensor.float() / 255.0\n",
        "    tensor = (tensor - 0.5) / 0.5  # Normalisasi seperti FaceNet\n",
        "\n",
        "    with torch.no_grad():\n",
        "        emb = model(tensor).cpu().numpy()[0]\n",
        "    return emb"
      ],
      "metadata": {
        "id": "jeEXnle9SNMe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Proses Dataset & Buat Database Embedding"
      ],
      "metadata": {
        "id": "NrXgK751SR-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mulai proses dataset dengan {AUG_PER_IMAGE}x augmentasi per foto...\\n\")\n",
        "\n",
        "database = {}      # {nama: [embeddings]}\n",
        "val_set = []       # (path, true_label)\n",
        "test_set = []      # (path, true_label)\n",
        "\n",
        "for person_name in sorted(os.listdir(DATASET_ROOT)):\n",
        "    person_path = os.path.join(DATASET_ROOT, person_name)\n",
        "    if not os.path.isdir(person_path):\n",
        "        continue\n",
        "\n",
        "    files = [f for f in os.listdir(person_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "\n",
        "    random.shuffle(files)\n",
        "    train_files = files[:min(9, len(files))]\n",
        "    val_files   = files[min(9, len(files)):min(12, len(files))]\n",
        "    test_files  = files[min(12, len(files)):min(14, len(files))]\n",
        "\n",
        "    print(f\"{person_name}: {len(train_files)} train → {len(train_files)*AUG_PER_IMAGE} embeddings | {len(val_files)} val | {len(test_files)} test\")\n",
        "\n",
        "    embeddings = []\n",
        "    for f in train_files:\n",
        "        path = os.path.join(person_path, f)\n",
        "        img_cv2 = cv2.imread(path)\n",
        "        if img_cv2 is None:\n",
        "            continue\n",
        "\n",
        "        # 1 embedding original + AUG_PER_IMAGE augmentasi\n",
        "        try:\n",
        "            emb_orig = get_embedding(img_cv2)\n",
        "            embeddings.append(emb_orig)\n",
        "            for _ in range(AUG_PER_IMAGE):\n",
        "                emb_aug = get_embedding(img_cv2)\n",
        "                embeddings.append(emb_aug)\n",
        "        except Exception as e:\n",
        "            print(f\"Error pada {f}: {e}\")\n",
        "\n",
        "    if len(embeddings) > 0:\n",
        "        database[person_name] = np.array(embeddings)\n",
        "\n",
        "    # Validation & Test (tanpa augmentasi)\n",
        "    for f in val_files:\n",
        "        val_set.append((os.path.join(person_path, f), person_name))\n",
        "    for f in test_files:\n",
        "        test_set.append((os.path.join(person_path, f), person_name))\n",
        "\n",
        "print(f\"\\nTotal orang: {len(database)}\")\n",
        "print(f\"Total embeddings train: {sum(len(v) for v in database.values())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rGme8IlSU2y",
        "outputId": "4c24d072-c283-4123-e869-18f568d61afa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mulai proses dataset dengan 50x augmentasi per foto...\n",
            "\n",
            "Aan Kristian Sitinjak: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Abeloisa Pardosi: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Adriano Lumbantoruan: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Agnes Sidabutar: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Andri Sigiro: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Angelika Simamora: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Anno Siregar: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Antonia Manalu: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Arnold Daniel Manalu: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Ayu Sinaga: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Bowo Manalu: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Chelsea Sianipar: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Chenit Sirait: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Cheryl: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Daniel Ginting: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Diva Marbun: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Estina Pangaribuan: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Febrina Sihombing: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Feby Manalu: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Febyanti Hutahaean: 9 train → 450 embeddings | 3 val | 0 test\n",
            "Febyola Hutagalung: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Felix Natanael Butarbutar: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Ferry Bastian Siagian: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Firman pane: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Franklyn Lumbantoruan: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Indra Aziz: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Irvan Agustriono Lumban Gaol: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Ize Sitorus: 9 train → 450 embeddings | 3 val | 2 test\n",
            "James Frans Rizky Tambunan: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Jesica Purba: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Jesica Sibarani: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Jesica Simanjuntak: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Jody Pangaribuan: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Junita Sihombing: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Kevin Rumapea: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Kezia Siahaan: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Lasro Tamba: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Marshanda Simangunsong: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Natasya Siahaan: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Nokatri Sitinjak: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Pedro Hutagaol: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Prapanca Silaen: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Putri Sihombing: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Rahel Pangaribuan: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Ricky Silaen: 9 train → 450 embeddings | 3 val | 1 test\n",
            "Rizky Apriyadi: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Romian Tambunan: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Roy Sigalingging: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Santo Martogi Simangunsong: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Tasya Simamora: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Valencia Tobing: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Vanesha Siahaan: 9 train → 450 embeddings | 3 val | 1 test\n",
            "Vinci Baringbing: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Wika Siregar: 9 train → 450 embeddings | 3 val | 2 test\n",
            "Winda Sembiring: 9 train → 450 embeddings | 3 val | 2 test\n",
            "\n",
            "Total orang: 55\n",
            "Total embeddings train: 25245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Simpan Database"
      ],
      "metadata": {
        "id": "_vLdtQsmSbPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names = list(database.keys())\n",
        "all_train_emb = np.vstack([database[n] for n in names])\n",
        "all_train_labels = np.array([n for n in names for _ in range(len(database[n]))], dtype=object)\n",
        "\n",
        "np.save(\"embeddings.npy\", all_train_emb)\n",
        "np.save(\"labels.npy\", all_train_labels)\n",
        "\n",
        "print(f\"Database berhasil disimpan!\")\n",
        "print(f\"→ Embeddings shape: {all_train_emb.shape}\")\n",
        "print(f\"→ Labels shape: {all_train_labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrcaTT-PShFH",
        "outputId": "ec4191ba-e47f-4479-ca7b-2db06a2d4807"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database berhasil disimpan!\n",
            "→ Embeddings shape: (25245, 512)\n",
            "→ Labels shape: (25245,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Tuning Threshold Otomatis\n",
        "Mencari threshold cosine similarity terbaik berdasarkan validation set"
      ],
      "metadata": {
        "id": "3tuJ0FzcS-pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tuning threshold ultra-presisi...\")\n",
        "\n",
        "def accuracy_at_threshold(th):\n",
        "    correct = 0\n",
        "    for path, true_name in val_set:\n",
        "        try:\n",
        "            emb = get_embedding(cv2.imread(path))\n",
        "            sims = np.dot(all_train_emb, emb) / (np.linalg.norm(all_train_emb, axis=1) * np.linalg.norm(emb))\n",
        "            pred_name = all_train_labels[np.argmax(sims)]\n",
        "            if sims.max() > th and pred_name == true_name:\n",
        "                correct += 1\n",
        "        except:\n",
        "            pass\n",
        "    return correct / len(val_set) if val_set else 0\n",
        "\n",
        "thresholds = np.round(np.arange(0.30, 0.80, 0.002), 3)\n",
        "accs = [accuracy_at_threshold(th) for th in tqdm(thresholds, desc=\"Tuning\")]\n",
        "\n",
        "best_th = thresholds[np.argmax(accs)]\n",
        "best_val_acc = max(accs) * 100\n",
        "\n",
        "print(f\"\\nThreshold terbaik: {best_th:.3f}\")\n",
        "print(f\"Validation Accuracy: {best_val_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCxFfB1WTDSe",
        "outputId": "5e1c3c5f-b17e-46f4-9a26-b9278f059167"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning threshold ultra-presisi...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tuning: 100%|██████████| 250/250 [1:03:48<00:00, 15.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Threshold terbaik: 0.300\n",
            "Validation Accuracy: 98.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Final Testing + Visualisasi Hasil"
      ],
      "metadata": {
        "id": "V9WTcX9CTFaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nFINAL TESTING dengan threshold = {best_th:.3f}\")\n",
        "\n",
        "correct = 0\n",
        "for path, true_name in tqdm(test_set, desc=\"Testing\"):\n",
        "    img_bgr = cv2.imread(path)\n",
        "    if img_bgr is None:\n",
        "        continue\n",
        "\n",
        "    emb = get_embedding(img_bgr)\n",
        "    sims = np.dot(all_train_emb, emb) / (np.linalg.norm(all_train_emb, axis=1) * np.linalg.norm(emb))\n",
        "    pred_name = all_train_labels[np.argmax(sims)]\n",
        "    score = sims.max()\n",
        "\n",
        "    status = \"BENAR\" if (score > best_th and pred_name == true_name) else \"SALAH\"\n",
        "    if status == \"BENAR\":\n",
        "        correct += 1\n",
        "\n",
        "    color = (0, 255, 0) if status == \"BENAR\" else (0, 0, 255)\n",
        "    cv2.putText(img_bgr, f\"{pred_name}\", (10, 50), cv2.FONT_HERSHEY_DUPLEX, 1.8, color, 3)\n",
        "    cv2.putText(img_bgr, f\"True: {true_name}\", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 0), 2)\n",
        "    cv2.putText(img_bgr, f\"{status} {score:.3f}\", (10, 170), cv2.FONT_HERSHEY_SIMPLEX, 1.4, color, 3)\n",
        "\n",
        "    filename = f\"{status}_{os.path.basename(path)}\"\n",
        "    cv2.imwrite(os.path.join(RESULT_FOLDER, filename), img_bgr)\n",
        "\n",
        "test_acc = correct / len(test_set) * 100\n",
        "print(f\"\\nAKURASI TEST FINAL: {correct}/{len(test_set)} = {test_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh7wjRmdTI16",
        "outputId": "b5eae369-deca-4c86-c4e0-6a9a7483e4d8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FINAL TESTING dengan threshold = 0.300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 106/106 [00:10<00:00, 10.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "AKURASI TEST FINAL: 101/106 = 95.28%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Simpan Laporan Lengkap"
      ],
      "metadata": {
        "id": "xAx1GgzQTLqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(RESULT_FOLDER, \"LAPORAN_MAX_ACC.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(f\"AUG_PER_IMAGE          : {AUG_PER_IMAGE}\\n\")\n",
        "    f.write(f\"Total orang            : {len(database)}\\n\")\n",
        "    f.write(f\"Total embeddings       : {all_train_emb.shape}\\n\")\n",
        "    f.write(f\"Validation Accuracy    : {best_val_acc:.2f}%\\n\")\n",
        "    f.write(f\"Test Accuracy          : {test_acc:.2f}%\\n\")\n",
        "    f.write(f\"Threshold optimal      : {best_th:.3f}\\n\")\n",
        "    f.write(f\"Use MTCNN Alignment    : {USE_MTCNN_ALIGN}\\n\")\n",
        "    f.write(f\"Dataset path           : {DATASET_ROOT}\\n\")\n",
        "\n",
        "print(\"SELESAI 1000%! yeyyy\")\n",
        "print(\"\\nFile yang dihasilkan:\")\n",
        "print(\" • embeddings.npy\")\n",
        "print(\" • labels.npy\")\n",
        "print(f\" • Folder: {RESULT_FOLDER}/ (gambar hasil + laporan)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA_wqvf1TOBJ",
        "outputId": "fe2dee6d-a8e6-4ad5-aced-26a18bac29ce"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SELESAI 1000%! yeyyy\n",
            "\n",
            "File yang dihasilkan:\n",
            " • embeddings.npy\n",
            " • labels.npy\n",
            " • Folder: hasil_validasi/ (gambar hasil + laporan)\n"
          ]
        }
      ]
    }
  ]
}